{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Passage Reranking with Permutation Self-Consistency\n",
        "\n",
        "This notebook implements the passage reranking experiment from the paper \"Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\" (arXiv:2310.07712).\n",
        "\n",
        "We use MS MARCO collection with TREC DL19/DL20 evaluation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Global Inputs**\n",
        "- Set your OpenAI API key here. If you're using Azure, see the code documentation for `OpenAIConfig` for how to modify it.\n",
        "- Set the aggregate size here. The paper uses 20 permutations for passage reranking.\n",
        "- Choose which TREC track to evaluate (DL19 or DL20).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = ''\n",
        "api_type = 'openai'  # or 'azure'\n",
        "num_aggregates = 20  # number of permutations (paper uses 20 for passage reranking)\n",
        "num_limit = 10  # number of queries to process (set to 200 for full DL19/DL20; 10 for testing)\n",
        "track = 'dl19'  # 'dl19' or 'dl20'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "# Fix for multiprocessing in Jupyter notebooks on macOS\n",
        "# Set start method to 'fork' if available, otherwise 'spawn'\n",
        "try:\n",
        "    mp.set_start_method('fork', force=True)\n",
        "except RuntimeError:\n",
        "    # Already set, or 'fork' not available (Windows)\n",
        "    pass\n",
        "\n",
        "from permsc import (\n",
        "    RelevanceRankingPromptBuilder,\n",
        "    OpenAIPromptPipeline,\n",
        "    OpenAIConfig,\n",
        "    ChatCompletionPool,\n",
        "    KemenyOptimalAggregator,\n",
        "    MSMarcoDataset,\n",
        "    ndcg_at_k,\n",
        "    mrr_at_k\n",
        ")\n",
        "\n",
        "config = OpenAIConfig(model_name='gpt-3.5-turbo', api_key=api_key, api_type=api_type)\n",
        "builder = RelevanceRankingPromptBuilder()\n",
        "pool = ChatCompletionPool([config] * 5)  # 5 parallel instances\n",
        "pipeline = OpenAIPromptPipeline(builder, pool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'494835'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset based on selected track\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m track \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdl19\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mMSMarcoDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/msmarco/collection.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/trec-dl19/msmarco-test2019-queries.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqrels_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/trec-dl19/2019qrels-pass.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop1000_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/trec-dl19/msmarco-passagetest2019-top1000.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_passages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m track \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdl20\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     ds \u001b[38;5;241m=\u001b[39m MSMarcoDataset(\n\u001b[1;32m     12\u001b[0m         collection_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/msmarco/collection.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m         queries_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/trec-dl20/msmarco-test2020-queries.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         max_passages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/diff_psc/permsc/data.py:281\u001b[0m, in \u001b[0;36mMSMarcoDataset.__init__\u001b[0;34m(self, collection_path, queries_path, qrels_path, top1000_path, max_passages, use_passage_from_top1000)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqrels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_qrels(qrels_path)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Load top-1000 retrieval results (qid -> list of (pid, query, passage, rank))\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_top1000\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop1000_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Create list of query IDs that have retrieval results\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_results\u001b[38;5;241m.\u001b[39mkeys())\n",
            "File \u001b[0;32m~/Documents/diff_psc/permsc/data.py:363\u001b[0m, in \u001b[0;36mMSMarcoDataset._load_top1000\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m qid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    361\u001b[0m         results[qid] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 363\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mappend((pid, query, passage, rank))\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parts[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m    365\u001b[0m     rank \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;31mKeyError\u001b[0m: '494835'"
          ]
        }
      ],
      "source": [
        "# Load dataset based on selected track\n",
        "if track == 'dl19':\n",
        "    ds = MSMarcoDataset(\n",
        "        collection_path='../data/msmarco/collection.tsv',\n",
        "        queries_path='../data/trec-dl19/msmarco-test2019-queries.tsv',\n",
        "        qrels_path='../data/trec-dl19/2019qrels-pass.txt',\n",
        "        top1000_path='../data/trec-dl19/msmarco-passagetest2019-top1000.tsv',\n",
        "        max_passages=100\n",
        "    )\n",
        "elif track == 'dl20':\n",
        "    ds = MSMarcoDataset(\n",
        "        collection_path='../data/msmarco/collection.tsv',\n",
        "        queries_path='../data/trec-dl20/msmarco-test2020-queries.tsv',\n",
        "        qrels_path='../data/trec-dl20/2020qrels-pass.txt',\n",
        "        top1000_path='../data/trec-dl20/msmarco-passagetest2020-top1000.tsv',\n",
        "        max_passages=100\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Unknown track: {track}\")\n",
        "\n",
        "print(f\"Dataset loaded: {len(ds)} queries\")\n",
        "print(f\"First query: {ds[0].query.content[:80]}...\")\n",
        "print(f\"First query has {len(ds[0].hits)} passages\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def run_passage_reranking_pipeline(pipeline, dataset, num_aggregates, limit=100):\n",
        "    \"\"\"\n",
        "    Run permutation self-consistency pipeline for passage reranking.\n",
        "    \n",
        "    Returns:\n",
        "        prefs_list: List of preference arrays (one per query, each with num_aggregates permutations)\n",
        "        perms_list: List of permutation arrays (input permutations used)\n",
        "        qrels_dict: Dictionary mapping query_id to qrels dict\n",
        "    \"\"\"\n",
        "    prefs_list = []\n",
        "    perms_list = []\n",
        "    qrels_dict = {}\n",
        "    \n",
        "    for example in dataset[:limit]:\n",
        "        example = deepcopy(example)\n",
        "        query_id = example.metadata.get('query_id')\n",
        "        qrels_dict[query_id] = example.metadata.get('qrels', {})\n",
        "        \n",
        "        prefs = []\n",
        "        items = []\n",
        "        perms = []\n",
        "        \n",
        "        # Generate num_aggregates permutations\n",
        "        for _ in range(num_aggregates):\n",
        "            ex_cpy = deepcopy(example)\n",
        "            perm = ex_cpy.randomize_order()\n",
        "            perms.append(perm)\n",
        "            items.append(ex_cpy)\n",
        "        \n",
        "        # Run pipeline\n",
        "        outputs = pipeline.run(items, temperature=0, request_timeout=30)\n",
        "        \n",
        "        # Restore preferences to original order\n",
        "        for output, perm_example in zip(outputs, items):\n",
        "            # Use the permuted example's restore method\n",
        "            restored_prefs = perm_example.permuted_preferences_to_original_order(output)\n",
        "            prefs.append(restored_prefs)\n",
        "        \n",
        "        prefs_list.append(np.array(prefs))\n",
        "        perms_list.append(np.array(perms))\n",
        "    \n",
        "    return prefs_list, perms_list, qrels_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_rankings(prefs_list):\n",
        "    \"\"\"Aggregate multiple preference rankings using Kemeny optimal aggregation.\"\"\"\n",
        "    aggregator = KemenyOptimalAggregator()\n",
        "    results = []\n",
        "    \n",
        "    for prefs in prefs_list:\n",
        "        aggregated = aggregator.aggregate(prefs)\n",
        "        results.append(aggregated)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_reranking(results, dataset, qrels_dict):\n",
        "    \"\"\"\n",
        "    Evaluate aggregated rankings using nDCG@10 and MRR@10.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with mean metrics and per-query scores\n",
        "    \"\"\"\n",
        "    ndcg_scores = []\n",
        "    mrr_scores = []\n",
        "    \n",
        "    for idx, (result, example) in enumerate(zip(results, dataset)):\n",
        "        query_id = example.metadata.get('query_id')\n",
        "        qrels = qrels_dict.get(query_id, {})\n",
        "        \n",
        "        # Convert preference array to ranked passage IDs\n",
        "        # Filter out -1 (missing items)\n",
        "        ranked_passage_ids = [example.hits[i].id for i in result if i != -1]\n",
        "        \n",
        "        # Compute metrics\n",
        "        ndcg = ndcg_at_k(ranked_passage_ids, qrels, k=10)\n",
        "        mrr = mrr_at_k(ranked_passage_ids, qrels, k=10)\n",
        "        \n",
        "        ndcg_scores.append(ndcg)\n",
        "        mrr_scores.append(mrr)\n",
        "    \n",
        "    return {\n",
        "        'ndcg@10': np.mean(ndcg_scores),\n",
        "        'mrr@10': np.mean(mrr_scores),\n",
        "        'ndcg_scores': ndcg_scores,\n",
        "        'mrr_scores': mrr_scores\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Permutation Self-Consistency Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run pipeline\n",
        "print(f\"Running pipeline on {num_limit} queries with {num_aggregates} permutations each...\")\n",
        "prefs_list, perms_list, qrels_dict = run_passage_reranking_pipeline(\n",
        "    pipeline, ds, num_aggregates, limit=num_limit\n",
        ")\n",
        "print(f\"Completed {len(prefs_list)} queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate rankings\n",
        "print(\"Aggregating rankings...\")\n",
        "results = aggregate_rankings(prefs_list)\n",
        "print(f\"Aggregated {len(results)} rankings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate aggregated results\n",
        "metrics = evaluate_reranking(results, ds[:num_limit], qrels_dict)\n",
        "print(f\"\\n=== Aggregated Results (PSC) ===\")\n",
        "print(f\"nDCG@10: {metrics['ndcg@10']:.4f}\")\n",
        "print(f\"MRR@10: {metrics['mrr@10']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with Baseline (First-Stage Retrieval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate baseline (original retrieval order)\n",
        "baseline_results = []\n",
        "for example in ds[:num_limit]:\n",
        "    # Use original retrieval order (passages are already in retrieval order)\n",
        "    ranked_ids = [hit.id for hit in example.hits]\n",
        "    baseline_results.append(ranked_ids)\n",
        "\n",
        "baseline_metrics = evaluate_reranking(baseline_results, ds[:num_limit], qrels_dict)\n",
        "print(f\"\\n=== Baseline (First-Stage Retrieval) ===\")\n",
        "print(f\"nDCG@10: {baseline_metrics['ndcg@10']:.4f}\")\n",
        "print(f\"MRR@10: {baseline_metrics['mrr@10']:.4f}\")\n",
        "\n",
        "print(f\"\\n=== Improvement ===\")\n",
        "print(f\"nDCG@10 improvement: {metrics['ndcg@10'] - baseline_metrics['ndcg@10']:.4f} ({((metrics['ndcg@10'] / baseline_metrics['ndcg@10'] - 1) * 100):.2f}%)\")\n",
        "print(f\"MRR@10 improvement: {metrics['mrr@10'] - baseline_metrics['mrr@10']:.4f} ({((metrics['mrr@10'] / baseline_metrics['mrr@10'] - 1) * 100):.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Individual Run Performance (without aggregation)\n",
        "\n",
        "Compare individual permutation runs to see the benefit of aggregation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_individual_runs(prefs_list, dataset, qrels_dict):\n",
        "    \"\"\"Evaluate each individual permutation run.\"\"\"\n",
        "    num_runs = len(prefs_list[0]) if prefs_list else 0\n",
        "    individual_ndcg = [[] for _ in range(num_runs)]\n",
        "    individual_mrr = [[] for _ in range(num_runs)]\n",
        "    \n",
        "    for idx, (prefs, example) in enumerate(zip(prefs_list, dataset)):\n",
        "        query_id = example.metadata.get('query_id')\n",
        "        qrels = qrels_dict.get(query_id, {})\n",
        "        \n",
        "        for run_idx, pref in enumerate(prefs):\n",
        "            # Convert preference array to ranked passage IDs\n",
        "            ranked_passage_ids = [example.hits[i].id for i in pref if i != -1]\n",
        "            \n",
        "            ndcg = ndcg_at_k(ranked_passage_ids, qrels, k=10)\n",
        "            mrr = mrr_at_k(ranked_passage_ids, qrels, k=10)\n",
        "            \n",
        "            individual_ndcg[run_idx].append(ndcg)\n",
        "            individual_mrr[run_idx].append(mrr)\n",
        "    \n",
        "    return {\n",
        "        'ndcg': [np.mean(scores) for scores in individual_ndcg],\n",
        "        'mrr': [np.mean(scores) for scores in individual_mrr]\n",
        "    }\n",
        "\n",
        "individual_metrics = evaluate_individual_runs(prefs_list, ds[:num_limit], qrels_dict)\n",
        "print(f\"Individual runs nDCG@10: {np.mean(individual_metrics['ndcg']):.4f} ± {np.std(individual_metrics['ndcg']):.4f}\")\n",
        "print(f\"Individual runs MRR@10: {np.mean(individual_metrics['mrr']):.4f} ± {np.std(individual_metrics['mrr']):.4f}\")\n",
        "print(f\"\\nAggregated nDCG@10: {metrics['ndcg@10']:.4f}\")\n",
        "print(f\"Aggregated MRR@10: {metrics['mrr@10']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
